\documentclass[12pt]{article}

% Bibliography preamble
%\usepackage{apacite}
\usepackage{cite}
%\usepackage[numbers,sort&compress]{natbib}

% Bullet preamble
\renewcommand{\labelitemi}{$\bullet$}
\renewcommand{\labelitemii}{$\diamond$}
\renewcommand{\labelitemiii}{$\circ$}

%\usepackage{lipsum}
\usepackage[margin=1in,left=1in,includefoot]{geometry}
\usepackage[hidelinks]{hyperref} % Allows for clickable references

\usepackage[none]{hyphenat} % Stop breaking up words in a table
\usepackage{array} % Allows for control of float positions
\newcolumntype{$}{>{\global\let\currentrowstyle\relax}}
\newcolumntype{^}{>{\currentrowstyle}}
\newcommand{\rowstyle}[1]{\gdef\currentrowstyle{#1}#1\ignorespaces}

%Graphics preamble
\usepackage{graphicx} % Allows you to import images
\usepackage{float} % Allows for control of float positions

% Math preamble
\usepackage{xfrac} % Allow for slanted fractions
\usepackage{physics}

% Header and Footer Stuff
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyfoot[R]{\thepage}
%\renewcommand{\headrulewidth}{1pt}
%\renewcommand{\footrulewidth}{1pt}

\begin{document}
%\title{Tabias, the Amazing Groundhog: The Report}
%\author{theZanshow}
%\date{June 26, 2014}
%\maketitle

\begin{titlepage}
	\begin{center}
	\line(1,0){500}\\
	[0.25in]
	\huge{\bfseries CAAM520 Computational Science}\\
	[2mm]
	\line(1,0){500}\\
	[1.5cm]
	\textsc{\LARGE Homework 3}\\
	[8cm] 
	\end{center}
	\begin{flushright}
	\textsc{\large AO CAI \\
	\ S01255664\\
	Earth Science Department\\
	Rice University\\
	March 5, 2019\\}
	\end{flushright}

\end{titlepage}

%Front matter stuff
\pagenumbering{roman}

%This is table of contents stuff
\tableofcontents
\thispagestyle{empty}
\cleardoublepage

%This is main body stuff
\pagenumbering{arabic}
\setcounter{page}{1}

\section{Introduction}\label{sec:intro}
The Goal is to solve the matrix system {\bfseries$Au=b$} resulting from an $(N+2)\times(N+2)$ 2D finite difference method for Laplace's equation $-\left(\pdv[2]{u}{x} + \pdv[2]{u}{y} \right) = f(x,y)$ on $[-1,1]^2$. At each point $(x_i,y_i)$, derivatives are approximated by:
$$\pdv[2]{u}{x} \approx \frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{h^2}, \pdv[2]{u}{y} \approx \frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{h^2}$$
where $h = 2/(N+1)$. Assuming zero boundary conditions $u(x,y) = 0$ reduce this to an $N \times N$ system for the interior nodes.

\section{Weighted Jacobi method with OPENMP}
To implement the parallel computing for the Poisson's equaion, I used the parallel for function in the openmp. First I compute the off-diagonal FD operators by using:
$$-Ru \approx \frac{-u_{i,j+1}-u_{i+1,j}-u_{i-1,j}-u_{i,j-1}}{h^2} $$
Then after this computing, the wavefield on each node is updated with
$$u^{(k+1)} = \omega D^{-1}(b-Ru^{(k)}) + (1-\omega)u^{(k)}$$
The code snippets for updating the wavefield is:
\begin{figure}[H]
	\centering
	\includegraphics[height=3.5in]{C:/Users/aocai/OneDrive/Desktop/weight_Jacob.JPG}
	\caption[Optional caption]{Weighted Jacobi method code snippets}
	\label{fig:wj}
\end{figure}
Figure 1 shows the snippets of the implementation of openmp based weighted Jacobi method, where $u$ is the wavefield, $b$ is the source function $f(x,y)$. The $Ru$ is first computed for the off-diagonal modeling operator, then the objective function is calculated and stored in $obj$. However, there is a race condition when computing the objective fuction. To solve the problem, I use the $reduction(+:obj)$ module to let the threads caches the sum results and put it back to a total sum after all the tasks on threads are finished.\\

In the numerical implementation, I show the examples by using different number of nodes on NOTS from 1 to 16. The solution is on the grid size of $N=200$
\begin{figure}[H]
	\centering
	\includegraphics[height=1.1in]{C:/Users/aocai/OneDrive/Desktop/Serial_Jacob.JPG}
	\caption[Optional caption]{Weigthted Jacobi method for N=200 Serial code}
	\label{JacobS}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.5in]{C:/Users/aocai/OneDrive/Desktop/Jacob1.JPG}
	\caption[Optional caption]{Weigthted Jacobi method for N=200 with 1 Thread}
	\label{Jacob1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.5in]{C:/Users/aocai/OneDrive/Desktop/Jacob2.JPG}
	\caption[Optional caption]{Weigthted Jacobi method for N=200 with 2 Threads}
	\label{Jacob2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.5in]{C:/Users/aocai/OneDrive/Desktop/Jacob4.JPG}
	\caption[Optional caption]{Weigthted Jacobi method for N=200 with 4 Threads}
	\label{Jacob3}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.5in]{C:/Users/aocai/OneDrive/Desktop/Jacob8.JPG}
	\caption[Optional caption]{Weigthted Jacobi method for N=200 with 8 Threads}
	\label{Jacob4}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.5in]{C:/Users/aocai/OneDrive/Desktop/Jacob16.JPG}
	\caption[Optional caption]{Weigthted Jacobi method for N=200 with 16 Threads}
	\label{Jacob4}
\end{figure}

\begin{table}[H]
	\centering
	\label{tab:cg}
	\begin{tabular}{lcr}
	\bfseries N {threads} & Computing time (s)\\ \hline
	Serial & 43.530\\
	1 & 6.827\\
	2 & 4.343\\
	4 & 2.933\\
	8 & 2.457\\
	16 & 2.706\\
	\end{tabular}
	\caption[This is optional caption, without reference]{Table for computing time using Weighted Jacobi method N=200}
\end{table}

Figure 2 to 7 shows the computing time by serial code and by using OPENMP with different number of threads. For relatively small threads value N<=4, we can see obvious improvements in the total computing time. The time is listed in the table 1, where we can see for large value of threads ($N = 8, 16$), the computing time stops decreasing and even raise a little bit.\\

For the OpenMP directives, I am using the pragma omp parallel for with the input number of threads. When compiling the program, I am using gcc -fopenmp -O3 myprog.c -o myprog. By using -fopenmp, I am using as may threads as available cores. The overall performance of the parallel program is good.
 
\section{GaussSeidel method with OPENMP}
The Gauss-Seidel method uses the following equations:
$${x_i}^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j=1}^{i-1}a_{ij}{x_j}^{(k+1)} -\sum_{j=i+1}^{n}a_{ij}{x_j}^{(k)} \right), i= 1,2,...,N$$
We first compute the product of upper triangular matrix U and wavefield u, and then compute the b-Ux. Finally, we update the solution u by computing the inverse of lower triangular matrix using forward substitution. The code snippets are provided below:

\begin{figure}[H]
	\centering
	\includegraphics[height=3.5in]{C:/Users/aocai/OneDrive/Desktop/GScode.JPG}
	\caption[Optional caption]{GaussSeidel method code snippets}
	\label{fig:gs}
\end{figure}

Figure 8 shows the snippets of the implementation of openmp based Gauss-Seidel method, where $u$ is the wavefield, $b$ is the source function $f(x,y)$. The $Uu$ is first computed for the Upper-triangle modeling operator, then wavefield $u$ is updated. However, there is a race condition when computing the objective fuction. To solve the problem, I use the $reduction(+:obj)$ module to let the threads caches the sum results and put it back to a total sum after all the tasks on threads are finished.\\

In the numerical implementation, I show the examples by using different number of nodes on NOTS from 1 to 16. The solution is on the grid size of $N=200$

% Here we insert our figure
\begin{figure}[H]
	\centering
	\includegraphics[height=1.1in]{C:/Users/aocai/OneDrive/Desktop/GSserial.JPG}
	\caption[Optional caption]{Gauss Seidel method on N=200, Serial code}
	\label{fig:GSs}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.5in]{C:/Users/aocai/OneDrive/Desktop/GS1.JPG}
	\caption[Optional caption]{Gauss Seidel method on N=200 with 1 Thread}
	\label{fig:GS1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.5in]{C:/Users/aocai/OneDrive/Desktop/GS2.JPG}
	\caption[Optional caption]{Gauss Seidel method on N=200 with 2 Threads}
	\label{fig:GS2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.5in]{C:/Users/aocai/OneDrive/Desktop/GS4.JPG}
	\caption[Optional caption]{Gauss Seidel method on N=200 with 4 Threads}
	\label{fig:GS4}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.5in]{C:/Users/aocai/OneDrive/Desktop/GS8.JPG}
	\caption[Optional caption]{Gauss Seidel method on N=200 with 8 Threads}
	\label{fig:GS8}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[height=1.5in]{C:/Users/aocai/OneDrive/Desktop/GS16.JPG}
	\caption[Optional caption]{Gauss Seidel method on N=200 with 16 Threads}
	\label{fig:GS16}
\end{figure}

\begin{table}[H]
	\centering
	\label{tab:gs}
	\begin{tabular}{lcr}
	\bfseries N {threads} & Computing time (s)\\ \hline
	Serial & 12.180\\
	1 & 2.735\\
	2 & 1.421\\
	4 & 0.779\\
	8 & 0.492\\
	16 & 0.395\\
	\end{tabular}
	\caption[This is optional caption, without reference]{Table for computing time using Gauss Seidel method N=200}
\end{table}
Figure 9 to 14 shows the computing time by serial code and by using OPENMP with different number of threads. For improving the number of threads, we can see obvious improvements in the total computing time. The time is listed in the table 2, where we can see the improvements from 8 threads to 16 threads is relatively smaller than from 1 thread to 2 threads.\\

For the OpenMP directives, I am using the pragma omp parallel for with the input number of threads. When compiling the program, I am using gcc -fopenmp -O3 myprog.c -o myprog. By using -fopenmp, I am using as may threads as available cores. The overall performance of the parallel program is good.

\section{Successive over-relaxation method with OPENMP}
The SOR method uses the following equations:
$${x_i}^{(k+1)} = (1-\omega){x_i}^{(k)} + \frac{\omega}{a_{ii}} \left( b_i - \sum_{j=1}^{i-1}a_{ij}{x_j}^{(k+1)} -\sum_{j=i+1}^{n}a_{ij}{x_j}^{(k)} \right), i= 1,2,...,N$$
Where the $omega$ is called the relaxation factor. The method goes back to Gauss-Seidel method when $\omega=0$. We first compute the product of upper triangular matrix U and wavefield u, and then compute the b-Ux. Finally, we update the solution u by computing the linear combination of updates from Gauss-Seidel method and solution u at last time step. The relaxation factor is $\omega = 0.9$. The code snippets are provided below:

\begin{figure}[H]
	\centering
	\includegraphics[height=3.5in]{C:/Users/aocai/OneDrive/Desktop/SORcode.JPG}
	\caption[Optional caption]{SOR method code snippets}
	\label{fig:sor}
\end{figure}

Figure 15 shows the snippets of the implementation of openmp based SOR method, where $u$ is the wavefield, $b$ is the source function $f(x,y)$. The $Uu$ is first computed for the Upper-triangle modeling operator, then wavefield $u$ is updated based on the weight factor. However, there is a race condition when computing the objective fuction. To solve the problem, I use the $reduction(+:obj)$ module to let the threads caches the sum results and put it back to a total sum after all the tasks on threads are finished.\\

In the numerical implementation, I show the examples by using different number of nodes on NOTS from 1 to 16. The solution is on the grid size of $N=200$

% Here we insert our figure
\begin{figure}[H]
	\centering
	\includegraphics[height=1.1in]{C:/Users/aocai/OneDrive/Desktop/SORserial.JPG}
	\caption[Optional caption]{SOR method on N=200, Serial code}
	\label{fig:SORs}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.5in]{C:/Users/aocai/OneDrive/Desktop/SOR1.JPG}
	\caption[Optional caption]{SOR method on N=200 with 1 Thread}
	\label{fig:SOR1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.5in]{C:/Users/aocai/OneDrive/Desktop/SOR2.JPG}
	\caption[Optional caption]{SOR method on N=200 with 2 Threads}
	\label{fig:SOR2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.5in]{C:/Users/aocai/OneDrive/Desktop/SOR4.JPG}
	\caption[Optional caption]{SOR method on N=200 with 4 Threads}
	\label{fig:SOR4}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.5in]{C:/Users/aocai/OneDrive/Desktop/SOR8.JPG}
	\caption[Optional caption]{SOR method on N=200 with 8 Threads}
	\label{fig:SOR8}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[height=1.5in]{C:/Users/aocai/OneDrive/Desktop/SOR16.JPG}
	\caption[Optional caption]{SOR method on N=200 with 16 Threads}
	\label{fig:SOR16}
\end{figure}

\begin{table}[H]
	\centering
	\label{tab:sor}
	\begin{tabular}{lcr}
	\bfseries N {threads} & Computing time (s)\\ \hline
	Serial & 13.350\\
	1 & 4.358\\
	2 & 2.509\\
	4 & 1.534\\
	8 & 1.190\\
	16 & 1.117\\
	\end{tabular}
	\caption[This is optional caption, without reference]{Table for computing time using SOR method N=200}
\end{table}
Figure 16 to 21 shows the computing time by serial code and by using OPENMP with different number of threads. For increasing the number of threads, we can see obvious improvements in the total computing time. The time is listed in the table 3, where we can see the improvements from 8 threads to 16 threads is relatively smaller than the improvements from 1 thread to 2 threads.\\

For the OpenMP directives, I am using the pragma omp parallel for with the input number of threads. When compiling the program, I am using gcc -fopenmp -O3 myprog.c -o myprog. By using -fopenmp, I am using as may threads as available cores. The overall performance of the parallel program is good.

\section{Conjugate Gradient method with OPENMP}
The algorithm is described below for solving our problem $Au=b$:
$$r_0 = b - Au_0 $$
$$p_0 = r_0 $$
$$k = 0$$
while $\left|| r_k \right|| >= tol $
$${\alpha}_k = \frac{{r_k}^T r_k}{{p_k}^T Ap_k}$$
$$u_{k+1} = u_k +{\alpha}_k p_k$$
$$r_{k+1} = r_k -{\alpha}_k Ap_k$$
$${\beta}_k = \frac{{r_{k+1}}^T{r_{k+1}}}{{r_k}^Tr_k}$$
$$p_{k+1} = r_{k+1} + \beta_kp_k$$
$$k=k+1$$
From the algorithm of CG method, we can expect that because the algorithm uses the gradient information and has approximate the Hessian matrix, if the equation is linear, it may take 1 iteration for the CG method to find out the solution. The code snippets are provided below:

\begin{figure}[H]
	\centering
	\includegraphics[height=3.5in]{C:/Users/aocai/OneDrive/Desktop/CGcode1.JPG}
	\includegraphics[height=2.5in]{C:/Users/aocai/OneDrive/Desktop/CGcode2.JPG}
	\caption[Optional caption]{Conjugate gradient method code snippets}
	\label{fig:sor}
\end{figure}
Figure 22 shows the snippets of the implementation of openmp based CG method, where $u$ is the wavefield, $b$ is the source function $f(x,y)$. The $\alpha$ and $\beta$ values are calculated in the corresponding 'compute'  function. However, there is a race condition when computing the objective fuction, alpha and beta. To solve the problem, I use the $reduction(+:obj)$, $reduction(+:alpha2)$, $reduction(+:beta0)$ module to let the threads caches the sum results and put it back to a total sum after all the tasks on threads are finished.
\begin{figure}[H]
	\centering
	\includegraphics[height=0.7in]{C:/Users/aocai/OneDrive/Desktop/CG200.JPG}
	\caption[Optional caption]{Conjugate Gradient method on N=200, Serial code}
	\label{fig:CG200}
\end{figure}
The CG method is superfast for N=200, as shown in Figure 23. In the numerical implementation, I show the examples by using different number of nodes on NOTS from 1 to 16. The solution is on the grid size of $N=5000$

\begin{figure}[H]
	\centering
	\includegraphics[height=0.7in]{C:/Users/aocai/OneDrive/Desktop/CG5000.JPG}
	\caption[Optional caption]{Conjugate Gradient method on N=5000, Serial code}
	\label{fig:CG5000}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.0in]{C:/Users/aocai/OneDrive/Desktop/CG1.JPG}
	\caption[Optional caption]{Conjugate Gradient method on N=5000 with 1 Thread}
	\label{fig:CG1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.0in]{C:/Users/aocai/OneDrive/Desktop/CG2.JPG}
	\caption[Optional caption]{Conjugate Gradient method on N=5000 with 2 Threads}
	\label{fig:CG2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.0in]{C:/Users/aocai/OneDrive/Desktop/CG4.JPG}
	\caption[Optional caption]{Conjugate Gradient method on N=5000 with 4 Threads}
	\label{fig:CG4}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.0in]{C:/Users/aocai/OneDrive/Desktop/CG8.JPG}
	\caption[Optional caption]{Conjugate Gradient method on N=5000 with 8 Threads}
	\label{fig:CG8}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[height=1.0in]{C:/Users/aocai/OneDrive/Desktop/CG16.JPG}
	\caption[Optional caption]{Conjugate Gradient method on N=5000 with 16 Threads}
	\label{fig:SOR16}
\end{figure}

\begin{table}[H]
	\centering
	\label{tab:sor}
	\begin{tabular}{lcr}
	\bfseries N {threads} & Computing time (s)\\ \hline
	Serial & 4.890\\
	1 & 1.901\\
	2 & 1.202\\
	4 & 0.832\\
	8 & 0.614\\
	16 & 0.523\\
	\end{tabular}
	\caption[This is optional caption, without reference]{Table for computing time using Conjugate Gradient method N=5000}
\end{table}

Figure 24 to 29 shows the computing time by serial code and by using OPENMP with different number of threads. For increasing the number of threads, we can see obvious improvements in the total computing time. The time is listed in the table 4, where we can see the improvements from 8 threads to 16 threads is relatively smaller than the improvements from 1 thread to 2 threads.\\

For the OpenMP directives, I am using the pragma omp parallel for with the input number of threads. When compiling the program, I am using gcc -fopenmp -O3 myprog.c -o myprog. By using -fopenmp, I am using as may threads as available cores. The overall performance of the parallel program is good.\\

In sum, for all the parallel program using openmp, I use the omp parallel for directives to let the threads parallel computing the for loops. When there is a summation in the for loop, there will be the race condition that the different threads might have the make the add function while the add operation in the other threads is not finished or the added sum is not put back to the shared variables. For this situation, the reduction() module is very useful to let different threads caches the sum at current steps.


%\bibliographystyle{IEEEtran}
%\bibliographystyle{apacite}
%\bibliography{References}
%\addcontentsline{toc}{section}{\numberline{}References}
%\cleardoublepage

\end{document}